{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pylab as plt\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "import os \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "import keras.utils\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"drive/My Drive/Train_Data/train_x.npy\")\n",
    "y_train = np.load(\"drive/My Drive/Train_Data/train_y.npy\")\n",
    "from sklearn.utils import shuffle\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(x_train)\n",
    "train_std = np.std(x_train)\n",
    "x_train = (x_train - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tensorflow.keras.utils.to_categorical(y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_classes = 3\n",
    "epochs = 50\n",
    "img_x, img_y = 512, 512\n",
    "input_shape = (img_x, img_y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception(include_top=True,\n",
    "             weights='imagenet',\n",
    "             input_tensor=None,\n",
    "             input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000,\n",
    "             **kwargs):\n",
    "    \"\"\"Instantiates the Xception architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    Note that the default input image size for this model is 299x299.\n",
    "    # Arguments\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(299, 299, 3)`.\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 71.\n",
    "            E.g. `(150, 150, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True,\n",
    "            and if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "        RuntimeError: If attempting to run this model with a\n",
    "            backend that does not support separable convolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_axis = -1\n",
    "\n",
    "    img_input=layers.Input(input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      strides=(2, 2),\n",
    "                      use_bias=False,\n",
    "                      name='block1_conv1')(img_input)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block1_conv1_bn')(x)\n",
    "    x = layers.Activation('relu', name='block1_conv1_act')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), use_bias=False, name='block1_conv2')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block1_conv2_bn')(x)\n",
    "    x = layers.Activation('relu', name='block1_conv2_act')(x)\n",
    "\n",
    "    residual = layers.Conv2D(128, (1, 1),\n",
    "                             strides=(2, 2),\n",
    "                             padding='same',\n",
    "                             use_bias=False)(x)\n",
    "    residual = layers.BatchNormalization(axis=channel_axis)(residual)\n",
    "\n",
    "    x = layers.SeparableConv2D(128, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block2_sepconv1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block2_sepconv1_bn')(x)\n",
    "    x = layers.Activation('relu', name='block2_sepconv2_act')(x)\n",
    "    x = layers.SeparableConv2D(128, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block2_sepconv2')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block2_sepconv2_bn')(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((3, 3),\n",
    "                            strides=(2, 2),\n",
    "                            padding='same',\n",
    "                            name='block2_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = layers.Conv2D(256, (1, 1), strides=(2, 2),\n",
    "                             padding='same', use_bias=False)(x)\n",
    "    residual = layers.BatchNormalization(axis=channel_axis)(residual)\n",
    "\n",
    "    x = layers.Activation('relu', name='block3_sepconv1_act')(x)\n",
    "    x = layers.SeparableConv2D(256, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block3_sepconv1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block3_sepconv1_bn')(x)\n",
    "    x = layers.Activation('relu', name='block3_sepconv2_act')(x)\n",
    "    x = layers.SeparableConv2D(256, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block3_sepconv2')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block3_sepconv2_bn')(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2),\n",
    "                            padding='same',\n",
    "                            name='block3_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = layers.Conv2D(728, (1, 1),\n",
    "                             strides=(2, 2),\n",
    "                             padding='same',\n",
    "                             use_bias=False)(x)\n",
    "    residual = layers.BatchNormalization(axis=channel_axis)(residual)\n",
    "\n",
    "    x = layers.Activation('relu', name='block4_sepconv1_act')(x)\n",
    "    x = layers.SeparableConv2D(728, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block4_sepconv1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block4_sepconv1_bn')(x)\n",
    "    x = layers.Activation('relu', name='block4_sepconv2_act')(x)\n",
    "    x = layers.SeparableConv2D(728, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block4_sepconv2')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block4_sepconv2_bn')(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2),\n",
    "                            padding='same',\n",
    "                            name='block4_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = layers.Activation('relu', name=prefix + '_sepconv1_act')(x)\n",
    "        x = layers.SeparableConv2D(728, (3, 3),\n",
    "                                   padding='same',\n",
    "                                   use_bias=False,\n",
    "                                   name=prefix + '_sepconv1')(x)\n",
    "        x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                      name=prefix + '_sepconv1_bn')(x)\n",
    "        x = layers.Activation('relu', name=prefix + '_sepconv2_act')(x)\n",
    "        x = layers.SeparableConv2D(728, (3, 3),\n",
    "                                   padding='same',\n",
    "                                   use_bias=False,\n",
    "                                   name=prefix + '_sepconv2')(x)\n",
    "        x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                      name=prefix + '_sepconv2_bn')(x)\n",
    "        x = layers.Activation('relu', name=prefix + '_sepconv3_act')(x)\n",
    "        x = layers.SeparableConv2D(728, (3, 3),\n",
    "                                   padding='same',\n",
    "                                   use_bias=False,\n",
    "                                   name=prefix + '_sepconv3')(x)\n",
    "        x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                      name=prefix + '_sepconv3_bn')(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = layers.Conv2D(1024, (1, 1), strides=(2, 2),\n",
    "                             padding='same', use_bias=False)(x)\n",
    "    residual = layers.BatchNormalization(axis=channel_axis)(residual)\n",
    "\n",
    "    x = layers.Activation('relu', name='block13_sepconv1_act')(x)\n",
    "    x = layers.SeparableConv2D(728, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block13_sepconv1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block13_sepconv1_bn')(x)\n",
    "    x = layers.Activation('relu', name='block13_sepconv2_act')(x)\n",
    "    x = layers.SeparableConv2D(1024, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block13_sepconv2')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block13_sepconv2_bn')(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((3, 3),\n",
    "                            strides=(2, 2),\n",
    "                            padding='same',\n",
    "                            name='block13_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = layers.SeparableConv2D(1536, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block14_sepconv1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block14_sepconv1_bn')(x)\n",
    "    x = layers.Activation('relu', name='block14_sepconv1_act')(x)\n",
    "\n",
    "    x = layers.SeparableConv2D(2048, (3, 3),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               name='block14_sepconv2')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, name='block14_sepconv2_bn')(x)\n",
    "    x = layers.Activation('relu', name='block14_sepconv2_act')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='xception')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'xception_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                TF_WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='0a58e3b7378bc2990ea3b43d5981f1f6')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'xception_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                TF_WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='b0042744bf5b25fce3cb969f33bebb97')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, blocks, name):\n",
    "    \"\"\"A dense block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, 32, name=name + '_block' + str(i + 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, reduction, name):\n",
    "    \"\"\"A transition block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        reduction: float, compression rate at transition layers.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 \n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                  name=name + '_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "    x = layers.Conv2D(int(backend.int_shape(x)[bn_axis] * reduction), 1,\n",
    "                      use_bias=False,\n",
    "                      name=name + '_conv')(x)\n",
    "    x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    \"\"\"A building block for a dense block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 \n",
    "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
    "                                   epsilon=1.001e-5,\n",
    "                                   name=name + '_0_bn')(x)\n",
    "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
    "                       use_bias=False,\n",
    "                       name=name + '_1_conv')(x1)\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                   name=name + '_1_bn')(x1)\n",
    "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "    x1 = layers.Conv2D(growth_rate, 3,\n",
    "                       padding='same',\n",
    "                       use_bias=False,\n",
    "                       name=name + '_2_conv')(x1)\n",
    "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def DenseNet(blocks,\n",
    "             include_top=True,\n",
    "             weights='imagenet',\n",
    "             input_tensor=None,\n",
    "             input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000,\n",
    "             **kwargs):\n",
    "    \"\"\"Instantiates the DenseNet architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        blocks: numbers of building blocks for the four dense layers.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `'channels_last'` data format)\n",
    "            or `(3, 224, 224)` (with `'channels_first'` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    bn_axis = 3 \n",
    "    img_input=layers.Input(input_shape)\n",
    "    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
    "    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv')(x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "    x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\n",
    "\n",
    "    x = dense_block(x, blocks[0], name='conv2')\n",
    "    x = transition_block(x, 0.5, name='pool2')\n",
    "    x = dense_block(x, blocks[1], name='conv3')\n",
    "    x = transition_block(x, 0.5, name='pool3')\n",
    "    x = dense_block(x, blocks[2], name='conv4')\n",
    "    x = transition_block(x, 0.5, name='pool4')\n",
    "    x = dense_block(x, blocks[3], name='conv5')\n",
    "\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\n",
    "    x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    if blocks == [6, 12, 24, 16]:\n",
    "        model = models.Model(inputs, x, name='densenet121')\n",
    "    elif blocks == [6, 12, 32, 32]:\n",
    "        model = models.Model(inputs, x, name='densenet169')\n",
    "    elif blocks == [6, 12, 48, 32]:\n",
    "        model = models.Model(inputs, x, name='densenet201')\n",
    "    else:\n",
    "        model = models.Model(inputs, x, name='densenet')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            if blocks == [6, 12, 24, 16]:\n",
    "                weights_path = keras_utils.get_file(\n",
    "                    'densenet121_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                    DENSENET121_WEIGHT_PATH,\n",
    "                    cache_subdir='models',\n",
    "                    file_hash='9d60b8095a5708f2dcce2bca79d332c7')\n",
    "            elif blocks == [6, 12, 32, 32]:\n",
    "                weights_path = keras_utils.get_file(\n",
    "                    'densenet169_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                    DENSENET169_WEIGHT_PATH,\n",
    "                    cache_subdir='models',\n",
    "                    file_hash='d699b8f76981ab1b30698df4c175e90b')\n",
    "            elif blocks == [6, 12, 48, 32]:\n",
    "                weights_path = keras_utils.get_file(\n",
    "                    'densenet201_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                    DENSENET201_WEIGHT_PATH,\n",
    "                    cache_subdir='models',\n",
    "                    file_hash='1ceb130c1ea1b78c3bf6114dbdfd8807')\n",
    "        else:\n",
    "            if blocks == [6, 12, 24, 16]:\n",
    "                weights_path = keras_utils.get_file(\n",
    "                    'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                    DENSENET121_WEIGHT_PATH_NO_TOP,\n",
    "                    cache_subdir='models',\n",
    "                    file_hash='30ee3e1110167f948a6b9946edeeb738')\n",
    "            elif blocks == [6, 12, 32, 32]:\n",
    "                weights_path = keras_utils.get_file(\n",
    "                    'densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                    DENSENET169_WEIGHT_PATH_NO_TOP,\n",
    "                    cache_subdir='models',\n",
    "                    file_hash='b8c4d4c20dd625c148057b9ff1c1176b')\n",
    "            elif blocks == [6, 12, 48, 32]:\n",
    "                weights_path = keras_utils.get_file(\n",
    "                    'densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                    DENSENET201_WEIGHT_PATH_NO_TOP,\n",
    "                    cache_subdir='models',\n",
    "                    file_hash='c13680b51ded0fb44dff2d8f86ac8bb1')\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def DenseNet121(include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                **kwargs):\n",
    "    return DenseNet([6, 12, 24, 16],\n",
    "                    include_top, weights,\n",
    "                    input_tensor, input_shape,\n",
    "                    pooling, classes,\n",
    "                    **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of DenseNet + Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model1 = DenseNet121(weights=None, input_shape=(512, 512, 3),include_top=False)\n",
    "model1  = Sequential()\n",
    "model1.add(base_model1)\n",
    "\n",
    "base_model2 = Xception(weights=None, input_shape=(512, 512, 3),include_top=False)\n",
    "model2  = Sequential()\n",
    "model2.add(base_model2)\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "x = concatenate([model1.output, model2.output], axis=-1)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "outputs = x\n",
    "model = Model(inputs=[model1.input, model2.input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x_train,x_train],y_train,epochs=epochs,batch_size=batch_size,class_weight=class_weight_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
